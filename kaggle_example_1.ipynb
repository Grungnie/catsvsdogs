{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This script goes along the blog post\n",
    "\"Building powerful image classification models using very little data\"\n",
    "from blog.keras.io.\n",
    "It uses data that can be downloaded at:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "In our setup, we:\n",
    "- created a data/ folder\n",
    "- created train/ and validation/ subfolders inside data/\n",
    "- created cats/ and dogs/ subfolders inside train/ and validation/\n",
    "- put the cat pictures index 0-999 in data/train/cats\n",
    "- put the cat pictures index 1000-1400 in data/validation/cats\n",
    "- put the dogs pictures index 12500-13499 in data/train/dogs\n",
    "- put the dog pictures index 13500-13900 in data/validation/dogs\n",
    "So that we have 1000 training examples for each class, and 400 validation examples for each class.\n",
    "In summary, this is our directory structure:\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```\n",
    "'''\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 10000\n",
    "nb_validation_samples = 2500\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.6707 - acc: 0.6120 - val_loss: 0.5623 - val_acc: 0.7048\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 21s 2ms/step - loss: 0.5827 - acc: 0.7041 - val_loss: 0.5204 - val_acc: 0.7456\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 21s 2ms/step - loss: 0.5350 - acc: 0.7422 - val_loss: 0.4952 - val_acc: 0.7724\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.5062 - acc: 0.7746 - val_loss: 0.6758 - val_acc: 0.7256\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4867 - acc: 0.7826 - val_loss: 0.5324 - val_acc: 0.7432\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4651 - acc: 0.7897 - val_loss: 0.4803 - val_acc: 0.7720\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 21s 2ms/step - loss: 0.4538 - acc: 0.7993 - val_loss: 0.4945 - val_acc: 0.7960\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4389 - acc: 0.8073 - val_loss: 0.4861 - val_acc: 0.7896\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4359 - acc: 0.8130 - val_loss: 0.5053 - val_acc: 0.7776\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4290 - acc: 0.8162 - val_loss: 0.4951 - val_acc: 0.7772\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4196 - acc: 0.8202 - val_loss: 0.4920 - val_acc: 0.7792\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4152 - acc: 0.8240 - val_loss: 0.4957 - val_acc: 0.7972\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4121 - acc: 0.8302 - val_loss: 0.4678 - val_acc: 0.7908\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4107 - acc: 0.8282 - val_loss: 0.4842 - val_acc: 0.7996\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4108 - acc: 0.8296 - val_loss: 0.5226 - val_acc: 0.7640\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4098 - acc: 0.8333 - val_loss: 0.4951 - val_acc: 0.7764\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3964 - acc: 0.8379 - val_loss: 0.5254 - val_acc: 0.7700\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4078 - acc: 0.8288 - val_loss: 0.4940 - val_acc: 0.7848\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4014 - acc: 0.8379 - val_loss: 0.5034 - val_acc: 0.7924\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3830 - acc: 0.8448 - val_loss: 0.4774 - val_acc: 0.7956\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4059 - acc: 0.8389 - val_loss: 0.4882 - val_acc: 0.7812\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3897 - acc: 0.8440 - val_loss: 0.5582 - val_acc: 0.7932\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3854 - acc: 0.8457 - val_loss: 0.9879 - val_acc: 0.7604\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3958 - acc: 0.8446 - val_loss: 0.5263 - val_acc: 0.8032\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3797 - acc: 0.8472 - val_loss: 0.5242 - val_acc: 0.7940\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3850 - acc: 0.8462 - val_loss: 0.5621 - val_acc: 0.8024\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3974 - acc: 0.8420 - val_loss: 0.5382 - val_acc: 0.7292\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3871 - acc: 0.8420 - val_loss: 0.7609 - val_acc: 0.7760\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3975 - acc: 0.8457 - val_loss: 0.4890 - val_acc: 0.7944\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3914 - acc: 0.8402 - val_loss: 0.5160 - val_acc: 0.8076\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4056 - acc: 0.8362 - val_loss: 0.6670 - val_acc: 0.7812\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3910 - acc: 0.8462 - val_loss: 0.4871 - val_acc: 0.7724\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4065 - acc: 0.8363 - val_loss: 0.4789 - val_acc: 0.7948\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4024 - acc: 0.8395 - val_loss: 0.6298 - val_acc: 0.8092\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4071 - acc: 0.8381 - val_loss: 0.5561 - val_acc: 0.7832\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4176 - acc: 0.8369 - val_loss: 0.5642 - val_acc: 0.7408\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.3949 - acc: 0.8413 - val_loss: 0.5773 - val_acc: 0.7760\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4133 - acc: 0.8408 - val_loss: 0.6118 - val_acc: 0.7772\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4155 - acc: 0.8384 - val_loss: 0.6059 - val_acc: 0.8008\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4221 - acc: 0.8352 - val_loss: 0.9744 - val_acc: 0.7320\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4166 - acc: 0.8309 - val_loss: 0.5341 - val_acc: 0.8124\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4042 - acc: 0.8388 - val_loss: 0.5134 - val_acc: 0.8004\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4195 - acc: 0.8326 - val_loss: 0.4913 - val_acc: 0.8096\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4073 - acc: 0.8392 - val_loss: 0.8411 - val_acc: 0.7620\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4171 - acc: 0.8405 - val_loss: 0.8461 - val_acc: 0.7724\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4218 - acc: 0.8344 - val_loss: 0.5186 - val_acc: 0.8048\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4120 - acc: 0.8413 - val_loss: 0.6244 - val_acc: 0.8196\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4308 - acc: 0.8333 - val_loss: 0.5775 - val_acc: 0.7924 0s - loss: 0.4265 - ac\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4224 - acc: 0.8300 - val_loss: 0.5169 - val_acc: 0.8008\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.4111 - acc: 0.8410 - val_loss: 0.5573 - val_acc: 0.7988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5be3bac10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=nb_train_samples,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=nb_validation_samples,\n",
    "    class_mode='binary')\n",
    "\n",
    "x_train, y_train = next(train_generator)\n",
    "x_val, y_val = next(validation_generator)\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "('Epoch', 0)\n",
      "image_processing: 82.0544009209\n",
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 21s 2ms/step - loss: 0.5743 - acc: 0.7456 - val_loss: 0.4605 - val_acc: 0.8076\n",
      "model_fitting: 21.4903039932\n",
      "('Epoch', 1)\n",
      "image_processing: 82.2597548962\n",
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 21s 2ms/step - loss: 0.5389 - acc: 0.7554 - val_loss: 0.4679 - val_acc: 0.8116\n",
      "model_fitting: 21.3692400455\n",
      "('Epoch', 2)\n",
      "image_processing: 82.5517599583\n",
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.5345 - acc: 0.7637 - val_loss: 0.4864 - val_acc: 0.7828\n",
      "model_fitting: 21.5315899849\n",
      "('Epoch', 3)\n",
      "image_processing: 83.1872169971\n",
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 22s 2ms/step - loss: 0.5456 - acc: 0.7588 - val_loss: 0.5483 - val_acc: 0.6952\n",
      "model_fitting: 21.6879029274\n",
      "('Epoch', 4)\n",
      "image_processing: 82.5178711414\n",
      "Train on 10000 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 21s 2ms/step - loss: 0.5207 - acc: 0.7684 - val_loss: 0.6414 - val_acc: 0.7492\n",
      "model_fitting: 21.3188169003\n",
      "('Epoch', 5)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-64078a7988e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# batches = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/preprocessing/image.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/preprocessing/image.pyc\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth_height_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1247\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/preprocessing/image.pyc\u001b[0m in \u001b[0;36mrandom_transform\u001b[0;34m(self, x, seed)\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mtransform_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_matrix_offset_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             x = apply_transform(x, transform_matrix, img_channel_axis,\n\u001b[0;32m--> 696\u001b[0;31m                                 fill_mode=self.fill_mode, cval=self.cval)\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_shift_range\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/preprocessing/image.pyc\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(x, transform_matrix, channel_axis, fill_mode, cval)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         cval=cval) for x_channel in x]\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/ndimage/interpolation.pyc\u001b[0m in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         _nd_image.geometric_transform(filtered, None, None, matrix, offset,\n\u001b[0;32m--> 486\u001b[0;31m                                       output, order, mode, cval, None, None)\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=nb_train_samples,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=nb_validation_samples,\n",
    "    class_mode='binary')\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "for e in range(epochs):\n",
    "    print('Epoch', e)\n",
    "    start = timer()\n",
    "    x_train, y_train = next(train_generator)\n",
    "    x_val, y_val = next(validation_generator)\n",
    "    # batches = 0\n",
    "    # for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n",
    "    mid = timer()\n",
    "    print('image_processing: {}'.format(mid-start))\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, validation_data=(x_val, y_val))\n",
    "    # batches += 1\n",
    "    # if batches >= len(x_train) / 32:\n",
    "    #     # we need to break the loop by hand because\n",
    "    #     # the generator loops indefinitely\n",
    "    #     break\n",
    "    end = timer()\n",
    "    print('model_fitting: {}'.format(end-mid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
